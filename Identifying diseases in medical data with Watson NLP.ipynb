{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying diseases in medical data using BERT\n",
    "In this notebook, we use IBM'S Watson NLP library to extract mentions to diseaes from the NCBI Disease dataset, hosted on [Hugging Face's repository](https://huggingface.co/datasets/ncbi_disease)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import watson_nlp\n",
    "import watson_nlp.data_model as dm\n",
    "from watson_nlp.toolkit import entity_mentions_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read and preprocess data\n",
    "### 2.1. Load data from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Huggingface repository\n",
    "ncbi = datasets.load_dataset('ncbi_disease')\n",
    "\n",
    "# Get train/test/val slices into dataframes\n",
    "df_train = pd.DataFrame(ncbi['train'])\n",
    "print('[+] Training set shape: {}'.format(df_train.shape))\n",
    "df_test = pd.DataFrame(ncbi['test'])\n",
    "print('[+] Testing set shape: {}'.format(df_test.shape))\n",
    "df_val = pd.DataFrame(ncbi['validation'])\n",
    "print('[+] Validation set shape: {}'.format(df_val.shape))\n",
    "\n",
    "list_df = [df_train, df_test, df_val]\n",
    "\n",
    "# Inspect\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Reshape data so that it can be used by Watson NLP\n",
    "\n",
    "The data does not come in a shape that's suitable for Watson NLP. We would like out data to be shaped in a json format such that each sentence is associated with a set of mentions (see cell below). We'll take several steps to reformat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(watson_nlp.toolkit.entity_mentions_utils.prepare_train_from_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Extract mentions from tokens column based on ner_tags column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def get_entities_from_token_list(list1, list2):\n",
    "    '''\n",
    "    Returns a list of labeled entities from a list of tokens and a list of ner tags. Params:\n",
    "    - list1: list of tokens\n",
    "    - list2: list of ner tags, taking values 0, 1 or 2\n",
    "    '''\n",
    "    \n",
    "    entities = list()\n",
    "    entity = ''\n",
    "    for token in range(len(list1)):\n",
    "        if list2[token] != 0:\n",
    "            if list2[token] == 1:\n",
    "                entity = list1[token]\n",
    "            elif list2[token] == 2:\n",
    "                entity = entity + ' ' + list1[token]\n",
    "            else:\n",
    "                '[-] NER indexing error!'\n",
    "        elif entity != '':\n",
    "            # When we no longer have an entity, we can append to entities list \n",
    "            entities.append(entity)\n",
    "            entity = ''\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to all 3 dataframes\n",
    "for _df in list_df:\n",
    "    _df['entities'] = [get_entities_from_token_list(_df['tokens'][i], _df['ner_tags'][i]) for i in range(len(_df))]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Transform tokens list into just a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def transform_tokens_into_sentence(list1):\n",
    "    '''\n",
    "    Returns a single string composed of all tokens in a list\n",
    "    '''\n",
    "    return \" \".join(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function\n",
    "for _df in list_df:\n",
    "    _df['sentence'] = [transform_tokens_into_sentence(_df['tokens'][i]) for i in range(len(_df))]\n",
    "\n",
    "df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Identify position of entities in sentences (beginning and end), and create json-like structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function - Get the position in the sentence where each entity begins and ends\n",
    "def get_entities_position(sentence:str, entities:list):\n",
    "    '''\n",
    "    For a given sentence containing a list of entities, it returns a list of dictionaries with the entity, its type (hardcoded to Disease) and its beginning and end. \n",
    "    - sentence: a sentence in string format\n",
    "    - entities: a list of entities that appear in sentence\n",
    "    '''\n",
    "\n",
    "    mentions = []\n",
    "    _sent = sentence\n",
    "    check = 0\n",
    "\n",
    "    for i in range(len(entities)):\n",
    "\n",
    "        _dict = {\n",
    "            'text':entities[i],\n",
    "            'type':\"Disease\",\n",
    "            'location':{}\n",
    "        }\n",
    "\n",
    "        b = _sent.find(entities[i]) + check\n",
    "        e = b + len(entities[i])\n",
    "\n",
    "        assert sentence[b:e] == entities[i]\n",
    "\n",
    "        _sent = sentence[e:] # This resets the string so that the find method starts looking after the last entity. Otherwise, if the same entity is repeated, it will always yield the first occurrence\n",
    "        check = len(sentence[:e])\n",
    "\n",
    "        _dict['location']['begin'] = b\n",
    "        _dict['location']['end'] = e\n",
    "\n",
    "        mentions.append(_dict)\n",
    "\n",
    "    return mentions\n",
    "\n",
    "\n",
    "# Define function - Produce a json-like structure with all entities' positions\n",
    "def build_json_structure(_df):\n",
    "    '''\n",
    "    Returns a json-like structure (a list of dictionaries) following the structure needed by a Watson NLU DataStream\n",
    "    '''\n",
    "\n",
    "    _list = []\n",
    "\n",
    "    for i in _df.index:\n",
    "        _dict = {\n",
    "            'id':i,\n",
    "            'text': _df['sentence'][i],\n",
    "        }\n",
    "\n",
    "        _dict['mentions'] = get_entities_position(_df['sentence'][i], _df['entities'][i])\n",
    "\n",
    "        _list.append(_dict)\n",
    "\n",
    "    return _list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our json-like structures\n",
    "train_list = build_json_structure(df_train)\n",
    "test_list = build_json_structure(df_test)\n",
    "val_list = build_json_structure(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as json\n",
    "out_file = open(\"./Data/train_set.json\", \"w\")     \n",
    "json.dump(train_list, out_file, indent = 4)    \n",
    "out_file.close() \n",
    "\n",
    "out_file = open(\"./Data/test_set.json\", \"w\")     \n",
    "json.dump(test_list, out_file, indent = 4)    \n",
    "out_file.close() \n",
    "\n",
    "out_file = open(\"./Data/val_set.json\", \"w\")     \n",
    "json.dump(val_list, out_file, indent = 4)    \n",
    "out_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Convert data to Watson NLP data streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download En syntax model\n",
    "syntax_model = watson_nlp.load(watson_nlp.download('syntax_izumo_en_stock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the entity labeled data in standard format to IOB streams\n",
    "train_labeled_data_stream = dm.DataStream.from_iterable(train_list)\n",
    "train_iob_stream = entity_mentions_utils.prepare_train_from_json(train_labeled_data_stream, syntax_model)\n",
    "\n",
    "val_labeled_data_stream = dm.DataStream.from_iterable(val_list)\n",
    "val_iob_stream = entity_mentions_utils.prepare_train_from_json(val_labeled_data_stream, syntax_model)\n",
    "\n",
    "test_labeled_data_stream = dm.DataStream.from_iterable(test_list)\n",
    "test_iob_stream = entity_mentions_utils.prepare_train_from_json(test_labeled_data_stream, syntax_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model\n",
    "\n",
    "### 3.1. Load pretrained model - BERT in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_resource = watson_nlp.load(watson_nlp.download('pretrained-model_bert_multi_bert_multi_cased'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train \n",
    "\n",
    "See method arguments below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(watson_nlp.blocks.entity_mentions.BERT.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = watson_nlp.toolkit.entity_mentions_utils.create_iob_labels(['Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model, returns the instance of the block\n",
    "entities_model = watson_nlp.blocks.entity_mentions.BERT.train(\n",
    "    train_labeled_documents = train_iob_stream,\n",
    "    dev_labeled_documents = val_iob_stream,\n",
    "    label_list = labels,\n",
    "    pretrained_model_resource = pretrained_model_resource,\n",
    "    do_lower_case=True,\n",
    "    num_train_epochs=4,\n",
    "    train_batch_size=32,\n",
    "    dev_batch_size=32,\n",
    "    keep_model_artifacts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = './Models/entities_bert'\n",
    "entities_model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Load model \n",
    "We need to read the model from local disk for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './Models/entities_bert'\n",
    "entities_model = watson_nlp.blocks.entity_mentions.BERT.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test model\n",
    "### 4.1. Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one quick test\n",
    "id = 1\n",
    "test_sentence = df_test['sentence'][id]\n",
    "test_entities = df_test['entities'][id]\n",
    "\n",
    "print('Sentence: ', test_sentence)\n",
    "print('Entities: ', test_entities)\n",
    "\n",
    "# Run syntax model on text\n",
    "syntax_analysis_en = syntax_model.run(test_sentence, parsers=('token',))\n",
    "# type(syntax_analysis_en), syntax_analysis_en.get_token_texts_by_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run BERT mentions model on syntax result\n",
    "ent_prediction = entities_model.run(syntax_analysis_en)\n",
    "ent_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the model and generate the quality report\n",
    "preprocess_func = lambda raw_doc: syntax_model.run(raw_doc)\n",
    "quality_report = entities_model.evaluate_quality('./Data/test_set.json', preprocess_func)\n",
    "\n",
    "# Print the quality report\n",
    "print(json.dumps(quality_report, indent=4))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc109d4e120f8e7603796211b6677ae82da12e688d8fcf28cc2e4e4249ba6ee3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('testenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
